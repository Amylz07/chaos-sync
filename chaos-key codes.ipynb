{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e4297-0c35-4875-bae9-8e1ba40e64e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages & definitions\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "from scipy.stats import rankdata\n",
    "import scipy as scipy\n",
    "import string\n",
    "import glob\n",
    "import warnings\n",
    "import math\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.path as mpath\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def ordinal_distribution_and_permutation_entropy(data, dx, dy, taux, tauy, return_missing=False, tie_precision=None):\n",
    "\n",
    "    try:\n",
    "        ny, nx = np.shape(data) \n",
    "        data   = np.array(data)\n",
    "    except:\n",
    "        nx     = np.shape(data)[0]\n",
    "        ny     = 1\n",
    "        data   = np.array([data])\n",
    "\n",
    "    if tie_precision is not None:\n",
    "        data = np.round(data, tie_precision)\n",
    "\n",
    "    partitions = np.concatenate(\n",
    "        [\n",
    "            [np.concatenate(data[j:j+dy*tauy:tauy,i:i+dx*taux:taux]) for i in range(nx-(dx-1)*taux)] \n",
    "            for j in range(ny-(dy-1)*tauy)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    symbols = np.apply_along_axis(rankdata, 1, partitions, method='min') - 1\n",
    "    symbols, symbols_count = np.unique(symbols, return_counts=True, axis=0)\n",
    "\n",
    "    probabilities = symbols_count/len(partitions)\n",
    "    \n",
    "    PE_raw=0\n",
    "    for i in range(len(symbols)-1):\n",
    "        PE_raw += -(probabilities[i]*math.log(probabilities[i],2))\n",
    "    PE_norm=PE_raw/math.log(np.math.factorial(dx),2)\n",
    "    \n",
    "    return symbols, probabilities, PE_norm\n",
    "    \n",
    "import scipy\n",
    "import scipy.signal\n",
    "\n",
    "def two_lorenz_odes(X, t):\n",
    "    x1, x2, x3, y1, y2, y3 = X\n",
    "    dx1 = sigma1*(x2-x1)+d1*(y1-x1)\n",
    "    dx2 = -x1*x3+rho1*x1-x2+d2*(y2-x2)\n",
    "    dx3 = x1*x2-beta1*x3+d3*(y3-x3)\n",
    "    dy1 = sigma2*(y2-y1)+d1*(x1-y1)\n",
    "    dy2 = -y1*y3+rho2*y1-y2+d2*(x2-y2)\n",
    "    dy3 = y1*y2-beta2*y3+d3*(x3-y3)\n",
    "    return (dx1, dx2, dx3, dy1, dy2, dy3)\n",
    "\n",
    "y0 = [0, 1, 0, 1, 1, 1]\n",
    "\n",
    "def IQR(data):\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    return iqr\n",
    "\n",
    "def time_series_graph(data):\n",
    "    time_series_graph = plt.figure(figsize=(20,5),dpi=200)\n",
    "    plt.plot(t, data[:len(data)], linewidth=1)\n",
    "    plt.xlabel(\"t\", fontsize=15)\n",
    "    plt.ylabel(\"{}\".format(data_name), fontsize=15)\n",
    "    plt.title(\"Lorenz Attractors Coupled, sigma1={}, rho1={}, beta1={}, sigma2={}, rho2={}, beta2={}, d1={}, d2={}, d3={}\".format(round(sigma1,2), round(rho1,2), round(beta1,2), round(sigma2,2), round(rho2,2), round(beta2,2), round(d1,2), round(d2,2), round(d3,2)))\n",
    "    return time_series_graph\n",
    "\n",
    "def peaks_list_def(data):\n",
    "    data_index = 0\n",
    "    turning_point_index_old = data_index\n",
    "    peaks_list = []\n",
    "    while data_index < len(data):\n",
    "        if data[data_index-1]*data[data_index]<0:\n",
    "            turning_point_index_new = data_index\n",
    "            peaks_raw=scipy.signal.find_peaks(data[turning_point_index_old:turning_point_index_new])\n",
    "            number_of_peaks=len(peaks_raw[0])\n",
    "            if data[data_index-1]>0 and data[data_index]<0:\n",
    "                peaks_list.append(number_of_peaks)\n",
    "            if data[data_index-1]<0 and data[data_index]>0:\n",
    "                peaks_list.append(number_of_peaks*(-1))\n",
    "            turning_point_index_old = turning_point_index_new\n",
    "            data_index += 1\n",
    "        else:\n",
    "            data_index += 1\n",
    "    return peaks_list\n",
    "\n",
    "def peaks_list_times_series_graph(peaks_list_data):\n",
    "    x_axis = list(range(len(peaks_list_data)))\n",
    "    y_axis = peaks_list_data\n",
    "    peaks_list_times_series_graph = plt.figure(figsize=(20,6),dpi=200)\n",
    "    plt.plot(x_axis, y_axis, linewidth=1)\n",
    "    plt.xlabel(\"sign change\", fontsize=10)\n",
    "    plt.ylabel(\"number of peaks\", fontsize=10)\n",
    "    plt.title(\"{} peak distribution, sigma1={}, rho1={}, beta1={}, sigma2={}, rho2={}, beta2={}, d1={}, d2={}, d3={}\".format(data_name, round(sigma1,2), round(rho1,2), round(beta1,2), round(sigma2,2), round(rho2,2), round(beta2,2), round(d1,2), round(d2,2), round(d3,2)))\n",
    "    return peaks_list_times_series_graph\n",
    "\n",
    "def peak_distribution_raw_data(peaks_list_data):\n",
    "    OD_and_PE=ordinal_distribution_and_permutation_entropy(peaks_list_data, dx=3, dy=1, taux=1, tauy=1)\n",
    "    ordinal_patterns=OD_and_PE[0].tolist()\n",
    "    probabilities=OD_and_PE[1].tolist()\n",
    "    OD_pattern_012=probabilities[0]\n",
    "    OD_pattern_210=probabilities[len(probabilities)-1]\n",
    "    ordinal_difference=OD_pattern_012-OD_pattern_210\n",
    "    permutation_entropy=OD_and_PE[2].tolist()\n",
    "    return ordinal_patterns, probabilities, ordinal_difference, permutation_entropy\n",
    "\n",
    "def my_autopct(pct):\n",
    "    return ('%.2f' % pct) if pct > 1 else ''\n",
    "\n",
    "def peak_ordinal_pattern_pie_chart(ordinal_patterns_data, probabilities_data):\n",
    "    data_pie={\"ordinal patterns\":ordinal_patterns_data, \"probabilities\":probabilities_data}\n",
    "    dataframe = pd.DataFrame(data_pie)\n",
    "    peak_ordinal_pattern_pie_chart = plt.figure(figsize =(10, 7),dpi=200) # plotting OD pie chart\n",
    "    plt.pie(probabilities_data, autopct=my_autopct, labels = ordinal_patterns_data)\n",
    "    plt.legend(ordinal_patterns, loc=\"center right\")\n",
    "    plt.title(\"{} peak distribution, sigma1={}, rho1={}, beta1={}, sigma2={}, rho2={}, beta2={}, d1={}, d2={}, d3={}\".format(data_name, round(sigma1,2), round(rho1,2), round(beta1,2), round(sigma2,2), round(rho2,2), round(beta2,2), round(d1,2), round(d2,2), round(d3,2)))\n",
    "    return peak_ordinal_pattern_pie_chart\n",
    "\n",
    "def peaks_composition_raw_data(peaks_list_data):\n",
    "    peaks_list_set=list(set(peaks_list_data))\n",
    "    peaks_list_set.sort()\n",
    "    index_peaks_list_set = 0\n",
    "    count_list = []\n",
    "    count_list_normalized = []\n",
    "    for number in peaks_list_set:\n",
    "        count_list.append(peaks_list.count(number))\n",
    "        count_list_normalized.append(peaks_list.count(number)/len(peaks_list_data))\n",
    "    return peaks_list_set, count_list_normalized\n",
    "\n",
    "def peaks_composition_complexity_def(count_list_normalized_data):\n",
    "    peaks_composition_complexity=0\n",
    "    for ele in count_list_normalized_data:\n",
    "        peaks_composition_complexity += ele*math.log(ele,2)\n",
    "    peaks_composition_complexity_normalized = peaks_composition_complexity/math.log(len(count_list_normalized_data),2)\n",
    "    return peaks_composition_complexity, peaks_composition_complexity_normalized\n",
    "\n",
    "def peaks_composition_pie_chart(peaks_list_set_data, count_list_normalized_data):\n",
    "    data_pie={\"number of peaks\":peaks_list_set_data, \"probabilities\":count_list_normalized_data}\n",
    "    dataframe = pd.DataFrame(data_pie)\n",
    "    peaks_composition_pie_chart = plt.figure(figsize =(10, 7),dpi=200)\n",
    "    centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    plt.pie(count_list_normalized_data, autopct=my_autopct, labels = peaks_list_set_data)\n",
    "    plt.legend(peaks_list_set_data, loc=\"best\")\n",
    "    plt.title(\"{} peaks composition, sigma1={}, rho1={}, beta1={}, sigma2={}, rho2={}, beta2={}, d1={}, d2={}, d3={}\".format(data_name, round(sigma1,2), round(rho1,2), round(beta1,2), round(sigma2,2), round(rho2,2), round(beta2,2), round(d1,2), round(d2,2), round(d3,2)))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05eedee-eebf-41f8-b1f0-7580442287d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system parameters setting\n",
    "sigma1 = 10\n",
    "rho1 = 50\n",
    "beta1 = 8/3\n",
    "sigma2 = 10\n",
    "rho2 = 40\n",
    "beta2 = 8/3\n",
    "tmax = 100\n",
    "\n",
    "t = np.arange(0.0, tmax, 0.001)\n",
    "\n",
    "x1_IQR_list = []\n",
    "x1_number_of_crossings_list = []\n",
    "x1_peak_PE_list = []\n",
    "x1_y1_IQR_list = []\n",
    "x1_y1_number_of_crossings_list = []\n",
    "x1_y1_peak_PE_list = []\n",
    "x1_y1_peaks_composition_complexity_list = []\n",
    "x1_y1_peaks_composition_complexity_normalized_list = []\n",
    "\n",
    "from itertools import repeat\n",
    "d1 = 0\n",
    "d2 = 0\n",
    "d1max = 10\n",
    "d2max = 10\n",
    "stepsize = 0.2\n",
    "d3 = 0\n",
    "d2_repeat_times = int((d2max-d2)/stepsize)\n",
    "d1_list_inner_layer = []\n",
    "d1_list_outer_layer = []\n",
    "d2_list = []\n",
    "\n",
    "while d2 < d2max:\n",
    "    while d1 < d1max:\n",
    "        d1_list_inner_layer.append(d1)\n",
    "        d1 += stepsize\n",
    "    d1_list_outer_layer.extend(d1_list_inner_layer)\n",
    "    d2_list.extend(repeat(d2, d2_repeat_times))\n",
    "    d2 += stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e08e30-7673-4c9d-8dfc-a78f85fcfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the algorithm\n",
    "k = 0\n",
    "while k<len(d1_list_outer_layer) and k<len(d2_list):\n",
    "    \n",
    "    d1=d1_list_outer_layer[k]\n",
    "    d2=d2_list[k]\n",
    "\n",
    "    X = odeint(two_lorenz_odes, y0, t, rtol=1e-6)\n",
    "    x1, x2, x3, y1, y2, y3 = X.T\n",
    "\n",
    "    # plt.figure(figsize=(5,5),dpi=200)\n",
    "    # plt.plot(x1[:len(x1)], y1[:len(y1)], linewidth=1)\n",
    "    # plt.xlabel(r'$x_1$', fontsize=15)\n",
    "    # plt.ylabel(r'$y_1$', fontsize=15)\n",
    "    # plt.title(\"sigma1={}, rho1={}, beta1={}, sigma2={}, rho2={}, beta2={}, d1={}, d2={}, d3={}\".format(round(sigma1,2), round(rho1,2), round(beta1,2), round(sigma2,2), round(rho2,2), round(beta2,2), round(d1,2), round(d2,2), round(d3,2)))\n",
    "    # plt.show()\n",
    "\n",
    "    data=x1-y1\n",
    "    data_name=\"x1-y1\"\n",
    "    # inter-quartile range\n",
    "    # print(\"{} IQR={}\".format(data_name, IQR(data)))\n",
    "    x1_y1_IQR_list.append(IQR(data))\n",
    "    # time series graph\n",
    "    # print(time_series_graph(data))\n",
    "    # peak list\n",
    "    peaks_list = peaks_list_def(data)\n",
    "    x1_y1_number_of_crossings_list.append(len(peaks_list))\n",
    "    # peak list time series graph\n",
    "    # print(peaks_list_times_series_graph(peaks_list))\n",
    "    # peak distribution PE and OD raw data\n",
    "    ordinal_patterns = peak_distribution_raw_data(peaks_list)[0]\n",
    "    probabilities = peak_distribution_raw_data(peaks_list)[1]\n",
    "    ordinal_difference = peak_distribution_raw_data(peaks_list)[2]\n",
    "    permutation_entropy = peak_distribution_raw_data(peaks_list)[3]\n",
    "    # peak distribution PE\n",
    "    # print(\"PE of {} peak distribution={}\".format(data_name, permutation_entropy))\n",
    "    x1_y1_peak_PE_list.append(permutation_entropy)\n",
    "    # peak distribution OD pie chart\n",
    "    # print(peak_ordinal_pattern_pie_chart(ordinal_patterns, probabilities))\n",
    "    # peak composition raw data\n",
    "    peaks_list_set = peaks_composition_raw_data(peaks_list)[0]\n",
    "    count_list_normalized = peaks_composition_raw_data(peaks_list)[1]\n",
    "    # peak composition complexity\n",
    "    x1_y1_peaks_composition_complexity = peaks_composition_complexity_def(count_list_normalized)[0]\n",
    "    x1_y1_peaks_composition_complexity_normalized = peaks_composition_complexity_def(count_list_normalized)[1]\n",
    "    x1_y1_peaks_composition_complexity_list.append(x1_y1_peaks_composition_complexity)\n",
    "    x1_y1_peaks_composition_complexity_normalized_list.append(x1_y1_peaks_composition_complexity_normalized)\n",
    "    # peak composition pie chart\n",
    "    # print(peaks_composition_pie_chart(peaks_list_set, count_list_normalized))\n",
    "\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa874c9f-0a2a-4658-a5f6-4697ca7b1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing data for heatmap\n",
    "x1_y1_IQR_matrix = np.array_split(x1_y1_IQR_list[:len(x1_y1_IQR_list)-50], d2_repeat_times)\n",
    "x1_y1_number_of_crossings_matrix = np.array_split(x1_y1_number_of_crossings_list[:len(x1_y1_IQR_list)-50], d2_repeat_times)\n",
    "x1_y1_peak_PE_matrix = np.array_split(x1_y1_peak_PE_list[:len(x1_y1_IQR_list)-50], d2_repeat_times)\n",
    "x1_y1_peaks_composition_complexity_matrix = np.array_split(x1_y1_peaks_composition_complexity_list[:len(x1_y1_IQR_list)-50], d2_repeat_times)\n",
    "x1_y1_peaks_composition_complexity_normalized_matrix = np.array_split(x1_y1_peaks_composition_complexity_normalized_list[:len(x1_y1_IQR_list)-50], d2_repeat_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a8c35-1e5c-48fc-8b12-a9db9d930919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap plotting\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = x1_y1_peaks_composition_complexity_normalized_matrix # edit the data and data_name to plot different heatmaps with the data stored above\n",
    "data_name = 'x1-y1 peaks composition complexity normalized'\n",
    "\n",
    "con_data = np.vstack(data)\n",
    "sn.set(rc = {'figure.figsize':(50, 50)})\n",
    "heatmap_data = sn.heatmap(data=con_data, annot=True, annot_kws={\"size\":20}, cmap=\"mako\")\n",
    "# d1 tick labels\n",
    "d1_ticklabels = 0\n",
    "list_d1_ticklabels = []\n",
    "while d1_ticklabels < 9.8:\n",
    "    list_d1_ticklabels.append('{:,}'.format(round(d1_ticklabels, 5)))\n",
    "    d1_ticklabels += 0.2\n",
    "# d2 tick labels\n",
    "d2_ticklabels = 0\n",
    "list_d2_ticklabels = []\n",
    "while d2_ticklabels < 9.8:\n",
    "    list_d2_ticklabels.append('{:,}'.format(round(d2_ticklabels, 5)))\n",
    "    d2_ticklabels += 0.2\n",
    "# heatmap plotting\n",
    "heatmap_data.set_xticklabels(list_d1_ticklabels, fontsize=25)\n",
    "heatmap_data.set_yticklabels(list_d2_ticklabels, fontsize=25)\n",
    "heatmap_data.invert_yaxis()\n",
    "heatmap_data.set_title('{}'.format(data_name), fontsize=60)\n",
    "heatmap_data.set_xlabel('d1', fontsize=60)\n",
    "heatmap_data.set_ylabel('d2', fontsize=60)\n",
    "cbar = heatmap_data.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=60)\n",
    "plt.show()\n",
    "print(heatmap_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
